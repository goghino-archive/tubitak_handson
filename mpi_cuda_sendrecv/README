This sample demonstrates how GPU device memory pointers can be used directly in MPI_Send/MPI_Recv calls. Works with OpenMPI implementation newer than 1.5.3 (in this 1.8.1).

jkardos@tesla-cmc:~/hands-on/mpi_cuda_sendrecv$ make

/home/jkardos/openmpi-1.8.1/install/bin/mpicc -g -std=c99 -I/opt/cuda/include -c mpi_cuda_sendrecv.c -o mpi_cuda_sendrecv.o

nvcc -g -c pattern2d.cu -o pattern2d.o

/home/jkardos/openmpi-1.8.1/install/bin/mpicc mpi_cuda_sendrecv.o pattern2d.o -o mpi_cuda_sendrecv -L/opt/cuda/lib64 -lcudart


/home/jkardos/openmpi-1.8.1/install/bin/mpirun -np 2 ./mpi_cuda_sendrecv 1024 2
4 CUDA device(s) found
Sending process 1 resulting field with average = 1.374382 to process 0
Sending process 0 resulting field with average = 1.350190 to process 1
Sending process 1 resulting field with average = 1.350190 to process 0
Sending process 0 resulting field with average = 1.374382 to process 1
